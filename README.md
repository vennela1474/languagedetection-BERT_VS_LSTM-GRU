# ğŸŒ Comparative Study: BERT vs. Hybrid LSTM-GRU for Language Detection

## ğŸ“„ Paper Title
**Comparative Study of BERT and Hybrid Model (LSTM+GRU) for Language Detection using NLP**

## ğŸ‘©â€ğŸ’» Authors
- **Kola Vennela** â€“ SR University  


## ğŸ“Œ Abstract

This study investigates two deep learning approaches for **multilingual language detection**:
- **HybridBERT**: A fine-tuned Multilingual BERT model  
- **Hybrid LSTM-GRU**: A combination of bidirectional recurrent networks

Both models were trained on a multilingual dataset spanning 17 languages.  
ğŸ“ˆ **Results:**
- **HybridBERT** achieved **99% accuracy**
- **LSTM-GRU Hybrid** achieved **95.6% accuracy**

This paper demonstrates the robustness of transformer-based models and the efficiency of hybrid RNNs in real-world, multilingual NLP tasks.


## ğŸ¯ Objectives

- To compare BERT and LSTM-GRU architectures for language detection.
- To evaluate their accuracy, F1-score, recall, and precision.
- To assess each model's computational efficiency and deployment feasibility.


## ğŸ“Š Results Summary

| Model             | Accuracy | Precision | Recall | F1-Score |
|------------------|----------|-----------|--------|----------|
| HybridBERT        | 99%      | 99%       | 99%    | 99%      |
| LSTM-GRU Hybrid   | 95.6%    | 96%       | 95%    | 95%      |


## ğŸ§  Key Features

- ğŸ”  Multilingual dataset with 17 languages
- ğŸ§ª Fine-tuned `bert-base-multilingual-cased` using HuggingFace Transformers
- ğŸ” Custom hybrid model built with BiLSTM and BiGRU layers
- ğŸ“‰ Precision-recall curves and confusion matrix visualizations
- ğŸ’¡ Lightweight alternative with LSTM-GRU for low-resource deployments


## ğŸ› ï¸ Tools & Frameworks

- Python
- TensorFlow / Keras
- HuggingFace Transformers
- Scikit-learn
- Matplotlib & Seaborn

